{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z90PtMhGZwm"
      },
      "source": [
        "# TradeSage AI Conversational User Interface (CUI) Prototype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpOiJZEEQnAV"
      },
      "source": [
        "## Install Dependencies\n",
        "Install all the required dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "500k-wBnFtdP"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install -U --quiet langchain langchain-core langchainhub langchain-community langchain-text-splitters langchain-openai google-genai langgraph langgraph-checkpoint-postgres psycopg psycopg-pool chromadb tiktoken pydantic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y7nvfJmREx8"
      },
      "source": [
        "## Import Packages\n",
        "Import all the required pacakges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVorMMmYRNHV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "from pprint import pprint\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import Image, Audio, display\n",
        "from typing_extensions import TypedDict, Dict, List, Literal, Annotated\n",
        "\n",
        "from langchain import hub\n",
        "from google.genai import Client\n",
        "from langchain_openai import ChatOpenAI\n",
        "from psycopg_pool import ConnectionPool\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "from langgraph.graph import START, MessagesState, StateGraph, END\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.messages import BaseMessage, SystemMessage, AIMessage, HumanMessage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz6kDwODSE0T"
      },
      "source": [
        "## Environment\n",
        "Create and Setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FS0Qm1vSTQv"
      },
      "outputs": [],
      "source": [
        "COING_API_KEY = os.getenv('COING_API_KEY')\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "QUANDL_API_KEY = os.getenv('QUANDL_API_KEY')\n",
        "LANGCHAIN_API_KEY = os.getenv('LANGCHAIN_API_KEY')\n",
        "\n",
        "LANGCHAIN_TRACING_V2: str = \"true\"\n",
        "LANGCHAIN_PROJECT: str = \"TradeSage\"\n",
        "AGENT_MODEL: str = \"gemini-2.0-flash-exp\"\n",
        "BASE_MESSAGE: BaseMessage = BaseMessage(type=\"base\", content=\"\"\"You are TradeSage AI, a highly advanced trading assistant designed to provide accurate, real-time market insights, personalized trading strategies, and portfolio management tools. Your role is to assist users with trading-related tasks in a professional yet approachable manner. You are integrated with real-time market data, sentiment analysis, backtesting tools, and educational resources. Hereâ€™s how you operate:\n",
        "\n",
        "1. **Market Insights**:\n",
        "   - Provide accurate, real-time data for stocks, forex, cryptocurrencies, and commodities.\n",
        "   - Offer technical analysis such as RSI, MACD, moving averages, and trendlines.\n",
        "   - Summarize news and its potential market impact.\n",
        "\n",
        "2. **Trading Strategies**:\n",
        "   - Suggest trading strategies based on user preferences and risk tolerance.\n",
        "   - Backtest strategies using historical data and present results with key metrics (e.g., ROI, Sharpe ratio).\n",
        "   - Provide educational explanations for technical indicators and strategy choices.\n",
        "\n",
        "3. **Sentiment Analysis**:\n",
        "   - Analyze social media and news sentiment to gauge market trends.\n",
        "   - Summarize relevant discussions from platforms like Twitter and Reddit.\n",
        "\n",
        "4. **Portfolio Management**:\n",
        "   - Track and evaluate user portfolios with performance metrics.\n",
        "   - Suggest diversification or rebalancing strategies to manage risk.\n",
        "\n",
        "5. **Tone and Interaction**:\n",
        "   - Be concise, professional, and friendly.\n",
        "   - Provide clear explanations for all recommendations.\n",
        "   - Acknowledge user inputs and clarify uncertainties.\n",
        "\n",
        "6. **Capabilities and Limitations**:\n",
        "   - Use APIs and tools to fetch live and historical data when queried.\n",
        "   - Always prioritize accuracy and relevance.\n",
        "   - Clearly indicate when data is unavailable or uncertain.\n",
        "\n",
        "### Examples of Interactions:\n",
        "- User: \"What's the price of Bitcoin now?\"\n",
        "  Response: \"Bitcoin is currently trading at $26,500, up 1.2% in the past 24 hours.\"\n",
        "\n",
        "- User: \"Suggest a trading strategy for Tesla.\"\n",
        "  Response: \"For Tesla, consider an RSI-based strategy: Buy when RSI drops below 30 and sell when it exceeds 70. Would you like me to backtest this?\"\n",
        "\n",
        "- User: \"How is the sentiment on Ethereum today?\"\n",
        "  Response: \"Sentiment on Ethereum is 80% bullish, driven by news of upcoming protocol upgrades.\"\n",
        "\n",
        "You are an AI and cannot provide financial or investment advice. Always remind users to make decisions based on their research and consult professionals where necessary. Aim to empower users with insights and tools to succeed in trading.\n",
        "\n",
        "\"\"\")\n",
        "SYSTEM_MESSAGE: SystemMessage = SystemMessage(type=\"system\", content=\"\"\"You are TradeSage AI, an advanced trading assistant capable of providing real-time market insights, strategy suggestions, sentiment analysis, and portfolio management assistance. Your role is to help users make informed trading decisions by leveraging live data, historical analysis, and AI-driven insights. Always maintain a professional, clear, and friendly tone. Here are your core capabilities:\n",
        "\n",
        "1. **Market Data & Analysis**:\n",
        "   - Provide up-to-date market data for stocks, cryptocurrencies, forex, and commodities.\n",
        "   - Offer technical analysis summaries, including indicators like RSI, MACD, and trendlines.\n",
        "   - Deliver concise news summaries and highlight their potential impact on the market.\n",
        "\n",
        "2. **Trading Strategy Assistance**:\n",
        "   - Recommend trading strategies tailored to user goals and risk levels.\n",
        "   - Perform backtesting of strategies using historical market data and present results with key metrics (e.g., return on investment, Sharpe ratio).\n",
        "   - Educate users by explaining the logic behind suggested strategies and technical indicators.\n",
        "\n",
        "3. **Sentiment Analysis**:\n",
        "   - Analyze and summarize market sentiment using data from social media and news sources.\n",
        "   - Indicate sentiment trends and potential implications for assets of interest.\n",
        "\n",
        "4. **Portfolio Management**:\n",
        "   - Provide an overview of user portfolios, including performance metrics and risk exposure.\n",
        "   - Suggest portfolio adjustments for diversification or risk management.\n",
        "   - Alert users about significant changes affecting their assets.\n",
        "\n",
        "5. **User Interaction**:\n",
        "   - Ensure responses are concise, data-backed, and informative.\n",
        "   - Use a professional yet approachable tone.\n",
        "   - Confirm user inputs, seek clarifications when necessary, and provide disclaimers as needed.\n",
        "\n",
        "6. **Capabilities**:\n",
        "   - Access real-time and historical market data through integrated APIs.\n",
        "   - Utilize machine learning models for predictive insights and sentiment analysis.\n",
        "   - Use a retrieval-augmented generation (RAG) system to pull relevant information from various sources.\n",
        "\n",
        "7. **Limitations**:\n",
        "   - You are an AI assistant and cannot provide personalized investment advice or make decisions for users.\n",
        "   - Always advise users to conduct their research or consult with a professional for investment decisions.\n",
        "\n",
        "**Example Interactions**:\n",
        "- User: \"What's the current price of Bitcoin?\"\n",
        "  Response: \"Bitcoin is currently trading at $26,500, up 1.2% over the past 24 hours.\"\n",
        "\n",
        "- User: \"Recommend a trading strategy for Ethereum.\"\n",
        "  Response: \"A common approach for Ethereum is using the RSI indicator. Buy when RSI drops below 30 and consider selling when it exceeds 70. Would you like me to backtest this for you?\"\n",
        "\n",
        "- User: \"How's the market sentiment for Tesla today?\"\n",
        "  Response: \"The current sentiment for Tesla is 75% positive, driven by strong quarterly results reported this morning.\"\n",
        "\n",
        "Remember to clearly state when data is unavailable or uncertain and prioritize transparency and user understanding in all interactions.\n",
        "\"\"\")\n",
        "DB_URI: str = \"postgresql://trading_agent_owner:6QuXEWimzLH7@ep-muddy-pine-a1h2nxln.ap-southeast-1.aws.neon.tech/trading_agent?sslmode=require\"\n",
        "\n",
        "CONNECTION_KWARGS = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
        "CONFIG = {\"configurable\": {\"thread_id\": \"1\"}, \"generation_config\": {\"response_modalities\": [\"TEXT\"]}, RunnableConfig: RunnableConfig}\n",
        "\n",
        "TEXT_SPLITTER: RecursiveCharacterTextSplitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=50)\n",
        "\n",
        "googleClient: Client = Client(\n",
        "  api_key=GOOGLE_API_KEY,\n",
        "  http_options= {'api_version': 'v1alpha'}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2Ydd1M4YZvH"
      },
      "source": [
        "## Memory, State & DB\n",
        "Create a memory, state & database to store data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "9mQLkwTKYj5N",
        "outputId": "368965ba-a02e-4457-98b5-0ffeffb58c5e"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected Embedings to be non-empty list or numpy array, got [] in upsert.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36m_validate_and_prepare_upsert_request\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         upsert_records = normalize_insert_record_set(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_insert_record_set\u001b[0;34m(ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     base_record_set = normalize_base_record_set(\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_base_record_set\u001b[0;34m(embeddings, documents, images, uris)\u001b[0m\n\u001b[1;32m    163\u001b[0m     return BaseRecordSet(\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_cast_one_to_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_embeddings\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;34mf\"Expected Embedings to be non-empty list or numpy array, got {target}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected Embedings to be non-empty list or numpy array, got []",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-63c43941a60e>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m coinData = Chroma.from_documents(\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoin_doc_splits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mcollection_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"coin_data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    888\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             ):\n\u001b[0;32m--> 843\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m    844\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 )\n\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             self._collection.upsert(\n\u001b[0m\u001b[1;32m    327\u001b[0m                 \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \"\"\"\n\u001b[0;32m--> 334\u001b[0;31m         upsert_request = self._validate_and_prepare_upsert_request(\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{str(e)} in {name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{str(e)} in {name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/models/CollectionCommon.py\u001b[0m in \u001b[0;36m_validate_and_prepare_upsert_request\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    387\u001b[0m     ) -> UpsertRequest:\n\u001b[1;32m    388\u001b[0m         \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         upsert_records = normalize_insert_record_set(\n\u001b[0m\u001b[1;32m    390\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m             \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_insert_record_set\u001b[0;34m(ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0mUnpacks\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnormalizes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfields\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0mInsertRecordSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \"\"\"\n\u001b[0;32m--> 187\u001b[0;31m     base_record_set = normalize_base_record_set(\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muris\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_base_record_set\u001b[0;34m(embeddings, documents, images, uris)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     return BaseRecordSet(\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_cast_one_to_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_cast_one_to_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/chromadb/api/types.py\u001b[0m in \u001b[0;36mnormalize_embeddings\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;34mf\"Expected Embedings to be non-empty list or numpy array, got {target}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected Embedings to be non-empty list or numpy array, got [] in upsert."
          ]
        }
      ],
      "source": [
        "# State\n",
        "class State(MessagesState):\n",
        "  userId: str\n",
        "  name: str\n",
        "  age: str\n",
        "  email: str\n",
        "  summary: str\n",
        "  portfoilo: dict\n",
        "\n",
        "\n",
        "# Memory\n",
        "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=CONNECTION_KWARGS)\n",
        "postgrees_checkpointer: PostgresSaver = PostgresSaver(pool)\n",
        "postgrees_checkpointer.setup()\n",
        "\n",
        "# Chroma DB\n",
        "coin_docs = \"\"\n",
        "news_docs = \"\"\n",
        "edu_docs = \"\"\n",
        "\n",
        "coin_doc_list = [item for sublist in coin_docs for item in sublist]\n",
        "news_doc_list = [item for sublist in news_docs for item in sublist]\n",
        "edu_doc_list = [item for sublist in edu_docs for item in sublist]\n",
        "\n",
        "coin_doc_splits = TEXT_SPLITTER.split_documents(coin_doc_list)\n",
        "news_docs_splits = TEXT_SPLITTER.split_documents(news_doc_list)\n",
        "edu_doc_splits = TEXT_SPLITTER.split_documents(edu_doc_list)\n",
        "\n",
        "\n",
        "coinData = Chroma.from_documents(\n",
        "  documents=coin_doc_splits,\n",
        "  collection_name=\"coin_data\",\n",
        "  embedding=OpenAIEmbeddings(\n",
        "    api_key=OPENAI_API_KEY\n",
        "  ),\n",
        ")\n",
        "\n",
        "newsData = Chroma.from_documents(\n",
        "  documents=news_docs_splits,\n",
        "  collection_name=\"news_data\",\n",
        "  embedding=OpenAIEmbeddings(\n",
        "    api_key=OPENAI_API_KEY\n",
        "  ),\n",
        ")\n",
        "\n",
        "eduData = Chroma.from_documents(\n",
        "  documents=edu_doc_splits,\n",
        "  collection_name=\"edu_data\",\n",
        "  embedding=OpenAIEmbeddings(\n",
        "    api_key=OPENAI_API_KEY\n",
        "  ),\n",
        ")\n",
        "\n",
        "coin_retiever = coinData.as_retriever()\n",
        "news_retiever = newsData.as_retriever()\n",
        "edu_retiever = eduData.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD6e8hdpZ24S"
      },
      "source": [
        "## Node (Tools/Functions) & Edges\n",
        "Create all the required functions to empower the agent to work with advance and complex tasks and create workflow functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Oy7_t2LaUr1"
      },
      "outputs": [],
      "source": [
        "def grade_documents(state:State) -> Literal[\"generate\", \"rewrite\"]:\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "    Args:\n",
        "      state (messages): The current state\n",
        "    Returns:\n",
        "      str: A decision for whether the documents are relevant or not\n",
        "    \"\"\"\n",
        "    class Grade(BaseModel):\n",
        "        \"\"\"Binary score for relevance check.\"\"\"\n",
        "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
        "\n",
        "    # LLM with structured output\n",
        "    llm = ChatOpenAI( api_key=OPENAI_API_KEY,\n",
        "        temperature=0, model=\"gpt-4-0125-preview\", streaming=True\n",
        "    ).with_structured_output(Grade)\n",
        "\n",
        "    # Prompt\n",
        "    prompt = PromptTemplate(\n",
        "      template=(\n",
        "        \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\\n\n",
        "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
        "        Here is the user question: {question} \\n\n",
        "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant to the question.\"\"\"\n",
        "      ),\n",
        "      input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "    # Chain\n",
        "    chain = prompt | llm\n",
        "\n",
        "    # Extract context and question\n",
        "    question = state[\"messages\"][0].content\n",
        "    docs = state[\"messages\"][-1].content\n",
        "\n",
        "    # Invoke chain\n",
        "    score = chain.invoke({\"question\": question, \"context\": docs}).binary_score\n",
        "\n",
        "    # Decision\n",
        "    if score == \"yes\":\n",
        "        print(\"---DECISION: DOCS RELEVANT---\")\n",
        "        return \"generate\"\n",
        "    else:\n",
        "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
        "        print(score)\n",
        "        return \"rewrite\"\n",
        "\n",
        "def rewrite(state:State):\n",
        "  \"\"\"\n",
        "  Transform the query to produce a better question.\n",
        "  Args:\n",
        "      state (messages): The current state\n",
        "  Returns:\n",
        "      dict: The updated state with re-phrased question\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---TRANSFORM QUERY---\")\n",
        "  messages = state[\"messages\"]\n",
        "  question = messages[0].content\n",
        "\n",
        "  msg = [\n",
        "    HumanMessage(\n",
        "      content=f\"\"\" \\n\n",
        "        Look at the input and try to reason about the underlying semantic intent / meaning. \\n\n",
        "        Here is the initial question:\n",
        "        \\n ------- \\n\n",
        "        {question}\n",
        "        \\n ------- \\n\n",
        "        Formulate an improved question:\n",
        "      \"\"\",\n",
        "    )\n",
        "  ]\n",
        "\n",
        "  # Grader\n",
        "  model = ChatOpenAI(api_key=OPENAI_API_KEY, temperature=0, model=\"gpt-4-0125-preview\",streaming=True)\n",
        "  response = model.invoke(msg)\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def generate(state:State):\n",
        "  \"\"\"\n",
        "  Generate answer\n",
        "  Args:\n",
        "    state (messages): The current state\n",
        "  Returns:\n",
        "    dict: The updated state with re-phrased question\n",
        "  \"\"\"\n",
        "  print(\"---GENERATE---\")\n",
        "  messages = state[\"messages\"]\n",
        "  question = messages[0].content\n",
        "  last_message = messages[-1]\n",
        "\n",
        "  docs = last_message.content\n",
        "\n",
        "  # Prompt\n",
        "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "  # LLM\n",
        "  llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)\n",
        "\n",
        "  # Post-processing\n",
        "  def format_docs(docs):\n",
        "      return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "  # Chain\n",
        "  rag_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "  # Run\n",
        "  response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "  return {\"messages\": [response]}\n",
        "\n",
        "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
        "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()\n",
        "\n",
        "def retriever_tool(retriever: str, re_name: str, re_desc: str):\n",
        "  return create_retriever_tool(retriever, re_name, re_desc)\n",
        "\n",
        "tools = ToolNode([retriever_tool])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA2le2pVaYpu"
      },
      "source": [
        "## Agent\n",
        "Create a Agent node that will be the brain and perform the tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51zS6R-Mao-G"
      },
      "outputs": [],
      "source": [
        "def call_module(state:State) -> Dict[List]:\n",
        "  \"\"\"\n",
        "    Invokes the agent model to generate a response based on the current state.Given the question, it will decide to retrieve using the retriever tool, or simply end.\n",
        "    Args:\n",
        "      state (messages): The current state\n",
        "    Returns:\n",
        "      dict: The updated state with the agent response appended to messages\n",
        "  \"\"\"\n",
        "  print(\"---CALL AGENT---\")\n",
        "  messages = state[\"messages\"]\n",
        "\n",
        "  role_mapping = {\n",
        "    \"base\": \"model\",\n",
        "    \"system\": \"model\",\n",
        "    \"human\": \"user\",\n",
        "    \"ai\": \"model\",\n",
        "  }\n",
        "  contents = [{\"role\": role_mapping.get(m.type, m.type), \"parts\": [{\"text\": m.content}]} for m in messages]\n",
        "  response = googleClient.models.generate_content(\n",
        "    model=AGENT_MODEL,\n",
        "    contents=contents\n",
        "  )\n",
        "  return {\"messages\": [response]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pNuq4c0a6QB"
      },
      "source": [
        "## Graph\n",
        "Create & compile the graph contaning nodes and edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsED43GRbNjz"
      },
      "outputs": [],
      "source": [
        "# Create\n",
        "workflow: StateGraph = StateGraph(State)\n",
        "\n",
        "# Node\n",
        "workflow.add_node(\"agent\", call_module)\n",
        "workflow.add_node(\"retrieve\", tools)\n",
        "workflow.add_node(\"rewrite\", rewrite)\n",
        "workflow.add_node(\"generate\", generate)\n",
        "\n",
        "# Edge\n",
        "workflow.add_edge(START, \"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "  \"agent\",\n",
        "  tools_condition,\n",
        "  {\n",
        "    \"tools\": \"retrieve\",\n",
        "    END: END,\n",
        "  },\n",
        ")\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"retrieve\",\n",
        "    grade_documents,\n",
        ")\n",
        "workflow.add_edge(\"generate\", END)\n",
        "workflow.add_edge(\"rewrite\", \"agent\")\n",
        "\n",
        "\n",
        "# Compile\n",
        "graph: CompiledStateGraph = workflow.compile(checkpointer=postgrees_checkpointer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mg6qoDgcZK4"
      },
      "source": [
        "## Display Graph\n",
        "Display the compiled graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1mQ26QhckBv"
      },
      "outputs": [],
      "source": [
        "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJRTbfL9csEo"
      },
      "source": [
        "## Start Conversation\n",
        "Start the conversation with the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4C-iBiPcxLC"
      },
      "outputs": [],
      "source": [
        "HUMAN_MESSAGE = HumanMessage(conent=\"\")\n",
        "\n",
        "graph.stream({\"messages\": HUMAN_MESSAGE}, CONFIG)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
